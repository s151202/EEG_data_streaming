{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('D:\\Diamond\\code')\n",
    "from csp_james_2 import *\n",
    "\n",
    "sys.path.append('D:\\Diamond\\code')\n",
    "from thesis_funcs_19_03 import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import csv\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "meth = 'gold_stand' #gold_standï¼Œtl_comp_csp_kld , tl_comp_csp_mi\n",
    "#raw_data_root = 'E:\\\\Diamond\\\\bci_iv\\\\DATA\\\\2a\\\\extract_raw\\\\'\n",
    "config_root= 'E:\\\\Diamond\\\\bci_iv\\\\MODELS\\\\fbcsp_mibif_cnn\\\\2a\\\\configs\\\\'\n",
    "\n",
    "feature_root = 'E:\\\\Diamond\\\\own_expo\\\\pilot_test\\\\'\n",
    "model_root = feature_root\n",
    "save_root = model_root + 'eval\\\\'\n",
    "\n",
    "\n",
    "#load in cv config grid\n",
    "hp_names  =[] #all the hyper-parameter names to be validated\n",
    "with open(config_root +'cv_config.csv', mode = 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "    for row in csv_reader:\n",
    "        hp_names.append((row[0]).strip())\n",
    "\n",
    "with open(config_root +'_lambda_config.csv', mode = 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "    for row in csv_reader:\n",
    "        hp_names.append((row[0]).strip())\n",
    "csv_file.close()\n",
    "\n",
    "\n",
    "\n",
    "num_inits = 5\n",
    "k_fold = 5\n",
    "\n",
    "# initialize csp\n",
    "m = 2# m is Nw in the paper \"learning temporal information for brain-copmuter interface, Sakhavi et.al\"\n",
    "n_components = 2 * m  # pick some components\n",
    "down_sample_step = 20 #Hilbert evelope\n",
    "# select Ns pairs of csp filters\n",
    "Ns = 4\n",
    "\n",
    "CLASSES =[0,1]\n",
    "\n",
    "C_OVR = [0,1]\n",
    "\n",
    "balance_classes = 1\n",
    "\n",
    "########################################################################################################################\n",
    "                                            # DEFINE FILTER BANK\n",
    "########################################################################################################################\n",
    "#Filter Bank\n",
    "FB = [[4., 8.], [8., 12.], [12., 16.], [16., 20.], [20., 24.], [24., 28.], [28., 32.], [32., 36.], [36., 40.]]\n",
    "FB = np.array(FB)\n",
    "\n",
    "#argumaents for Chebyl II filtering\n",
    "# Nyquist frequency\n",
    "\n",
    "# min. attenuation in stop band\n",
    "gstop = 45\n",
    "# max. attenuation in passband\n",
    "gpass= 5\n",
    "\n",
    "EEG_PERIOD = [[0.5,4]]\n",
    "FS = [512]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Diamond\\\\own_expo\\\\pilot_test\\\\eval\\\\'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_root  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_train_eval_ind(sub_id):\n",
    "    EEG_MI_RAW_load = pickle.load(open( 'E:\\\\Diamond\\\\own_expo\\\\pilot_test\\\\' + sub_id + '\\\\signals\\\\' + sub_id + \".pickle\", \"rb\" ) )\n",
    "    LABELS_load = pickle.load(open( 'E:\\\\Diamond\\\\own_expo\\\\pilot_test\\\\' + sub_id + '\\\\signals\\\\' + sub_id + \"_LABELS.pickle\", \"rb\" ) )\n",
    "\n",
    "    #fist half of recording is training, second half is evaluation\n",
    "    train_set_ind = np.arange(0, int(len(LABELS_load)/2))\n",
    "    eval_set_ind = np.arange(int(len(LABELS_load)/2), int(len(LABELS_load)))\n",
    "\n",
    "    EEG_MI_RAW_T = EEG_MI_RAW_load[train_set_ind]\n",
    "    EEG_MI_RAW_E = EEG_MI_RAW_load[eval_set_ind]\n",
    "    LABELS_load_T = LABELS_load[train_set_ind]\n",
    "    LABELS_load_E = LABELS_load[eval_set_ind]\n",
    "    \n",
    "    return EEG_MI_RAW_T, LABELS_load_T, EEG_MI_RAW_E, LABELS_load_E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 [0.93333333 0.73333333] 0.8333333333333333\n"
     ]
    }
   ],
   "source": [
    "for portion_train in [1]:\n",
    "    if to_save == 1:\n",
    "        filewrite = open(save_root + '4s_' + str(int(portion_train*100))+'_best_config_eval_acc_all_subjects.txt', 'w')\n",
    "        filewrite.write('')\n",
    "        filewrite.close()\n",
    "\n",
    "        filewrite = open(save_root + '4s_' + str(int(portion_train*100))+'_best_config_eval_acc_all_subjects.txt', 'a')\n",
    "        filewrite.write('subject, ')\n",
    "        for f in range (0, len(C_OVR)-1):\n",
    "            filewrite.write('class '+ str(C_OVR[f]+1) + ', ')\n",
    "        filewrite.write('class ' + str(C_OVR[-1]+1) + ', average'+ ', best_model_init_fold\\n')\n",
    "\n",
    "        \n",
    "    for subject in range (3,4):\n",
    "\n",
    "        sub_id = 's' + str(subject)\n",
    "\n",
    "        eeg_period = EEG_PERIOD[0]\n",
    "        fs = FS[0]\n",
    "   \n",
    "        \n",
    "        for run_win in range (0,1):\n",
    "            if run_win == 0:\n",
    "                file_root_feature =  feature_root + sub_id + '\\\\models\\\\4s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                file_root_model = model_root + sub_id + '\\\\models\\\\4s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                #file_root_save = save_root + filename_save[:-1] + '\\\\4s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                #len_inp = 44\n",
    "            elif run_win == 1:\n",
    "                file_root_feature = feature_root + filename_save[:-1] + '\\\\2s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                file_root_model = model_root + filename_save[:-1] + '\\\\2s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                #file_root_save = save_root + filename_save[:-1] + '\\\\2s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                #len_inp = 25\n",
    "                \n",
    "            ###################################################################################################################\n",
    "                            #load best config\n",
    "            ###################################################################################################################\n",
    "            #load in best config line\n",
    "            config_file = open(file_root_model + '\\\\ANN\\\\best_config_val.txt', 'r')\n",
    "            config_log= config_file.readlines()\n",
    "            config_file.close()\n",
    "            for i in range (0,len(config_log)):\n",
    "                line  = config_log[(i + 1) * -1]\n",
    "                if '_act_fun_' in line: #and  line.split(' ')[0].split('_lambda_')[1] == '0':\n",
    "                    break\n",
    "\n",
    "            #extract best config values and make into dictionary\n",
    "            config = OrderedDict()\n",
    "            for hp_ind in range(0, len(hp_names)-1):\n",
    "                config[hp_names[hp_ind]] =  (line.split(hp_names[hp_ind] + '_')[1].split('_'+hp_names[hp_ind+1]+'_')[0])\n",
    "            config[hp_names[-1]] = line.split(hp_names[-1]+'_')[1].split(' ')[0]\n",
    "            \n",
    "            \n",
    "            \n",
    "            EEG_MI_RAW_T, LABELS_load_T, EEG_MI_RAW_E, LABELS_load_E= seperate_train_eval_ind(sub_id)\n",
    "            LABELS = LABELS_load_E - 1\n",
    "            EEG_extract_raw = EEG_MI_RAW_E\n",
    "            ########################################################################################################################\n",
    "                                        #APPLY FILTER BANK\n",
    "            ########################################################################################################################\n",
    "            #Store Filter bank filtered raw EEG data, in the shape of num_filter_bank X num_trials X num_chanl X num_samples\n",
    "            #initiate empty matrix\n",
    "            EEG_filt_FB_L = np.empty( [len(FB), \n",
    "                                     np.shape(EEG_extract_raw)[0], np.shape(EEG_extract_raw)[1],np.shape(EEG_extract_raw)[2]] )\n",
    "\n",
    "            Nf = fs / 2.\n",
    "            for fb in range (0, len(FB)):\n",
    "                passband = FB[fb]\n",
    "                stopband = FB[fb] + np.array([-2., +2.])\n",
    "\n",
    "                EEG_filt_FB_L[fb] = filter_signal(EEG_extract_raw, passband, stopband, Nf, gpass, gstop)\n",
    "\n",
    "\n",
    "            EEG_filt_FB = EEG_filt_FB_L\n",
    "            \n",
    "            #trake only the MI 3.5 seconds\n",
    "            EEG_filt_FB_go = EEG_filt_FB[:,:,:,int(eeg_period[0]*fs):int(eeg_period[1]*fs)]\n",
    "            LABELS0_go = LABELS.copy()\n",
    "            LABELS0 = LABELS0_go\n",
    "            \n",
    "            ###########################################################################################################################\n",
    "    \n",
    "            pred_indi = []\n",
    "            ############################################################################################################################\n",
    "            OUT = 0\n",
    "            \n",
    "            best_mod_acc_prod = 0 #initialise best model average class acc\n",
    "            best_mod_kappa = -2\n",
    "            best_model = [0,0] #which model performs the best? model id, init = best_model[0], fold = best_model[1]\n",
    "    \n",
    "            for fold in range (0, k_fold):\n",
    "                #print ('fold', fold)\n",
    "\n",
    "                pred_indi.append([])\n",
    "                \n",
    "                for c_ovr in C_OVR:\n",
    "                    #print (c_ovr)\n",
    "                    #load in csp filters and mutual informtaion ranked indicies\n",
    "                    W_B = pickle.load(open( file_root_feature +'\\\\W_B_fold_' + str(fold) + \n",
    "                                               '_c_ovr_' + str(c_ovr) + '_lambda_' + str(float(config['_lambda'])) + \n",
    "                                               \".pickle\", 'rb'))\n",
    "\n",
    "\n",
    "                    FB_FILTER_IND = pickle.load(open( file_root_feature +  '\\\\FB_FILTER_IND_fold_' + str(fold) + \n",
    "                                               '_c_ovr_' + str(c_ovr) + '_lambda_' + str(float(config['_lambda'])) + \n",
    "                                               \".pickle\", 'rb'))\n",
    "\n",
    "                    #find the selected csp filters indicies\n",
    "                    FB_FILTER_IND_slt = find_selected_csp_filters(Ns, m, FB_FILTER_IND)\n",
    "\n",
    "                    #construct selected csp filters, W_B_slt has shape (2*Ns, num_chls), (8,22) for example\n",
    "                    W_B_slt = W_B[FB_FILTER_IND_slt[:,0], :, FB_FILTER_IND_slt[:,1]]\n",
    "\n",
    "                    EEG_FB_slt = EEG_filt_FB_go[FB_FILTER_IND_slt[:,0],:]\n",
    "\n",
    "                    #transform into z space, then take the hilbert envelope of the transformed signal\n",
    "                    Z_env = calc_z_features(W_B_slt, EEG_FB_slt, Ns, down_sample_step)\n",
    "\n",
    "                    #concatenate all classes\n",
    "                    if c_ovr == C_OVR[0]:\n",
    "                        Z_all_eval = Z_env\n",
    "                    else:\n",
    "                        Z_all_eval = np.concatenate((Z_all_eval, Z_env), axis = 0)\n",
    "\n",
    "                #reshape into ANN input size        \n",
    "                Z_all_eval = np.transpose(Z_all_eval, [1,0,2])\n",
    "                X_eval = np.reshape(Z_all_eval, [np.shape(Z_all_eval)[0], 1, np.shape(Z_all_eval)[1], np.shape(Z_all_eval)[2]])\n",
    "                X_eval = torch.from_numpy(X_eval).float()\n",
    "                \n",
    "                #initilize ANN model\n",
    "                model = Model_current(chn_inp = X_eval.size()[-2], len_inp = X_eval.size()[-1], nf = int(config['nf']), ks = int(config['ks']) , \n",
    "                                  stride = int(config['stride']), act_f = config['act_fun'], nfc = int(config['nfc']))\n",
    "                \n",
    "                for n_inits in range (0, num_inits):\n",
    "                    save_path = file_root_model + '\\\\ANN\\\\model_config_'+ line.split(' ')[0] + '_n_inits_'+ str(n_inits) +'_fold_' + str(fold) + '.pt'\n",
    "                    model.load_state_dict(torch.load(save_path))\n",
    "                    model.eval()\n",
    "\n",
    "                    #predictoin, sum up the output (probability of being class) predicted at each fold, tehn the class with the max probability if the class prediction\n",
    "                    out = model(X_eval)\n",
    "                    OUT = OUT + out\n",
    "\n",
    "                    #print out class precition at each fold\n",
    "                    pred = torch.argmax(out, dim = 1).numpy()\n",
    "                    #print (str(n_inits), np.average(calc_class_acc(pred, LABELS0, C_OVR)))\n",
    "                    if cohen_kappa_score(LABELS0, pred) > best_mod_kappa:\n",
    "                        best_model = [n_inits, fold]\n",
    "                        best_mod_acc_prod = np.average(calc_class_acc(pred, LABELS0, C_OVR))\n",
    "                        best_mod_kappa = cohen_kappa_score(LABELS0, pred)\n",
    "\n",
    "                    #pred_indi[fold].append(cohen_kappa_score(LABELS0, pred))\n",
    "                    \n",
    "                    \n",
    "            #final prediciotn using all trained ANNs   \n",
    "            PRED = torch.argmax(OUT, dim = 1).numpy()\n",
    "            acc_c = calc_class_acc(PRED, LABELS0, C_OVR)\n",
    "            print(sub_id, acc_c, np.average(acc_c))\n",
    "            kappa = cohen_kappa_score(LABELS0, PRED)\n",
    "            \n",
    "            if to_save == 1:\n",
    "                filewrite.write(str(subject) + ', ')\n",
    "\n",
    "                for a in acc_c:\n",
    "                    filewrite.write(str(round(a*100,2)) + ', ')\n",
    "                filewrite.write(str(round(np.average(acc_c)*100, 2)) +' '+'('+str(round(kappa, 3))+')' +', ' + str(best_model[0])+'_'+str(best_model[1]) + '_'+ str(best_mod_acc_prod) + '(' + str(best_mod_kappa) + ')'  +'\\n')\n",
    "                \n",
    "                \n",
    "    if to_save == 1:\n",
    "        filewrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
